{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    \"\"\"\n",
    "        处理加载的训练数据DataFrame，去掉id，同时对signals以”，“进行拆分。\n",
    "    :param data: DataFrame, shape(n, 3)\n",
    "    :return: np array, shape(n, 206)\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for i in range(data.shape[0]):\n",
    "        x_res = data.iloc[i, 1].split(',')\n",
    "        label = data.iloc[i, 2]\n",
    "        x_res.append(label)\n",
    "        res.append(x_res)\n",
    "    return np.array(res, dtype=np.float64)\n",
    "\n",
    "\n",
    "def get_pred_x(data):\n",
    "    \"\"\"\n",
    "        处理需要预测数据的DataFrame\n",
    "    :param data: DataFrame, shape(n, 2)\n",
    "    :return: np array, shape(n, 205)\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for i in range(data.shape[0]):\n",
    "        x_res = data.iloc[i, 1].split(',')\n",
    "        res.append(x_res)\n",
    "    return np.array(res, dtype=np.float64)\n",
    "\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "        模型训练\n",
    "    :param dataloader: 训练数据集\n",
    "    :param model: 训练用到的模型\n",
    "    :param loss_fn: 评估用的损失函数\n",
    "    :param optimizer: 优化器\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for batch, x_y in enumerate(dataloader):\n",
    "        X, y = x_y[:, :205].type(torch.float64), torch.tensor(x_y[:, 205], dtype=torch.long, device='cuda:0')\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # Compute prediction and loss\n",
    "            pred = model(X.float())\n",
    "            loss = loss_fn(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    \"\"\"\n",
    "        模型测试部分\n",
    "    :param dataloader: 测试数据集\n",
    "    :param model: 测试模型\n",
    "    :param loss_fn: 损失函数\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct, l1_loss = 0, 0, 0\n",
    "    # 用来计算abs-sum. 等于PyTorch L1Loss\n",
    "    l1loss_fn = AbsSumLoss()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for x_y in dataloader:\n",
    "            X, y = x_y[:, :205].type(torch.float64), torch.tensor(x_y[:, 205], dtype=torch.long, device='cuda:0')\n",
    "            # Y用来计算L1 loss, y是CrossEntropy loss.\n",
    "            Y = torch.zeros(size=(len(y), 4), device='cuda:0')\n",
    "            for i in range(len(Y)):\n",
    "                Y[i][y[i]] = 1\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            l1_loss += l1loss_fn(pred, Y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Results:\\nAccuracy: {(100 * correct):>0.1f}% abs-sum loss: {l1_loss:>8f} CroEtr loss: {test_loss:>8f}\")\n",
    "    \n",
    "def prediction(net):\n",
    "    \"\"\"\n",
    "        对数据进行预测\n",
    "    :param net: 训练好的模型\n",
    "    :param loss: 模型的测试误差值, 不是损失函数. 可以去掉, 这里是用来给预测数据命名方便区分.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        pred_loader = torch.utils.data.DataLoader(dataset=pred_data)\n",
    "        res = []\n",
    "        for x in pred_loader:\n",
    "            x = torch.tensor(x, device='cuda:0', dtype=torch.float64)\n",
    "            output = net(x.float())\n",
    "            res.append(output.cpu().numpy().tolist())\n",
    "\n",
    "        res = [i[0] for i in res]\n",
    "        res_df = pd.DataFrame(res, columns=['label_0', 'label_1', 'label_2', 'label_3'])\n",
    "        res_df.insert(0, 'id', value=range(100000, 120000))\n",
    "\n",
    "        res_df.to_csv('CNN-final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv_layer1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1),  # 16, 1, 205\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_layer2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size=5, stride=4),                                 # 32, 1, 51\n",
    "        )\n",
    "\n",
    "        self.conv_layer3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1),  # 64, 1, 51\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_layer4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size=3, stride=2),                                 # 128, 1, 25\n",
    "        )\n",
    "\n",
    "        self.conv_layer5 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_layer6 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size=3, stride=2),                                  # 512, 1, 12\n",
    "        )\n",
    "\n",
    "        self.full_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=512 * 12, out_features=512 * 12),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512 * 12, out_features=512 * 12),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512 * 12, out_features=4)\n",
    "        )\n",
    "\n",
    "        self.pred_layer = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(dim=1)\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        x = self.conv_layer3(x)\n",
    "        x = self.conv_layer4(x)\n",
    "        x = self.conv_layer5(x)\n",
    "        x = self.conv_layer6(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.full_layer(x)\n",
    "\n",
    "        if self.training:\n",
    "            return x\n",
    "        else:\n",
    "            return self.pred_layer(x)\n",
    "\n",
    "\n",
    "class AbsSumLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AbsSumLoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss = F.l1_loss(target, output, reduction='sum')\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------Epoch 1----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/admin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Accuracy: 98.1% abs-sum loss: 1201.772060 CroEtr loss: 0.769396\n",
      "training time:  24.242589712142944\n",
      "\n",
      "----------Epoch 2----------\n",
      "Test Results:\n",
      "Accuracy: 97.9% abs-sum loss: 989.339923 CroEtr loss: 0.765845\n",
      "training time:  24.239996671676636\n",
      "\n",
      "----------Epoch 3----------\n",
      "Test Results:\n",
      "Accuracy: 98.5% abs-sum loss: 1039.282774 CroEtr loss: 0.765441\n",
      "training time:  24.23212456703186\n",
      "\n",
      "----------Epoch 4----------\n",
      "Test Results:\n",
      "Accuracy: 98.6% abs-sum loss: 704.752886 CroEtr loss: 0.759143\n",
      "training time:  24.197970390319824\n",
      "\n",
      "----------Epoch 5----------\n",
      "Test Results:\n",
      "Accuracy: 98.7% abs-sum loss: 719.777543 CroEtr loss: 0.759363\n",
      "training time:  24.198222637176514\n",
      "\n",
      "----------Epoch 6----------\n",
      "Test Results:\n",
      "Accuracy: 98.8% abs-sum loss: 593.445000 CroEtr loss: 0.756768\n",
      "training time:  24.20291018486023\n",
      "\n",
      "----------Epoch 7----------\n",
      "Test Results:\n",
      "Accuracy: 98.9% abs-sum loss: 651.028199 CroEtr loss: 0.757639\n",
      "training time:  24.2035014629364\n",
      "\n",
      "----------Epoch 8----------\n",
      "Test Results:\n",
      "Accuracy: 98.6% abs-sum loss: 836.242959 CroEtr loss: 0.761594\n",
      "training time:  24.236000299453735\n",
      "\n",
      "----------Epoch 9----------\n",
      "Test Results:\n",
      "Accuracy: 98.9% abs-sum loss: 541.197609 CroEtr loss: 0.755654\n",
      "training time:  24.195910453796387\n",
      "\n",
      "----------Epoch 10----------\n",
      "Test Results:\n",
      "Accuracy: 98.9% abs-sum loss: 562.474890 CroEtr loss: 0.755969\n",
      "training time:  24.190031051635742\n",
      "\n",
      "----------Epoch 11----------\n",
      "Test Results:\n",
      "Accuracy: 99.1% abs-sum loss: 474.049288 CroEtr loss: 0.754246\n",
      "training time:  24.20427942276001\n",
      "\n",
      "----------Epoch 12----------\n",
      "Test Results:\n",
      "Accuracy: 99.1% abs-sum loss: 444.757722 CroEtr loss: 0.753682\n",
      "training time:  24.206439971923828\n",
      "\n",
      "----------Epoch 13----------\n",
      "Test Results:\n",
      "Accuracy: 99.0% abs-sum loss: 474.992906 CroEtr loss: 0.754467\n",
      "training time:  24.202634811401367\n",
      "\n",
      "----------Epoch 14----------\n",
      "Test Results:\n",
      "Accuracy: 99.1% abs-sum loss: 419.412959 CroEtr loss: 0.753233\n",
      "training time:  24.20863151550293\n",
      "\n",
      "----------Epoch 15----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 367.657056 CroEtr loss: 0.752210\n",
      "training time:  24.201854705810547\n",
      "\n",
      "----------Epoch 16----------\n",
      "Test Results:\n",
      "Accuracy: 99.1% abs-sum loss: 404.477900 CroEtr loss: 0.752933\n",
      "training time:  24.209919452667236\n",
      "\n",
      "----------Epoch 17----------\n",
      "Test Results:\n",
      "Accuracy: 99.1% abs-sum loss: 406.381230 CroEtr loss: 0.752976\n",
      "training time:  24.21155571937561\n",
      "\n",
      "----------Epoch 18----------\n",
      "Test Results:\n",
      "Accuracy: 99.1% abs-sum loss: 387.136845 CroEtr loss: 0.752635\n",
      "training time:  24.21756935119629\n",
      "\n",
      "----------Epoch 19----------\n",
      "Test Results:\n",
      "Accuracy: 99.1% abs-sum loss: 369.156802 CroEtr loss: 0.752322\n",
      "training time:  24.19526433944702\n",
      "\n",
      "----------Epoch 20----------\n",
      "Test Results:\n",
      "Accuracy: 99.1% abs-sum loss: 390.003962 CroEtr loss: 0.752786\n",
      "training time:  24.21364164352417\n",
      "\n",
      "----------Epoch 21----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 343.783803 CroEtr loss: 0.751697\n",
      "training time:  15.987438201904297\n",
      "\n",
      "----------Epoch 22----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 325.038438 CroEtr loss: 0.751233\n",
      "training time:  15.998407363891602\n",
      "\n",
      "----------Epoch 23----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 320.585937 CroEtr loss: 0.751154\n",
      "training time:  15.987375497817993\n",
      "\n",
      "----------Epoch 24----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 320.576658 CroEtr loss: 0.751139\n",
      "training time:  15.997867584228516\n",
      "\n",
      "----------Epoch 25----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 322.990746 CroEtr loss: 0.751216\n",
      "training time:  15.996941566467285\n",
      "\n",
      "----------Epoch 26----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 318.440572 CroEtr loss: 0.751125\n",
      "training time:  15.998064517974854\n",
      "\n",
      "----------Epoch 27----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 315.872462 CroEtr loss: 0.751060\n",
      "training time:  15.994700193405151\n",
      "\n",
      "----------Epoch 28----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 311.939011 CroEtr loss: 0.750980\n",
      "training time:  15.992814302444458\n",
      "\n",
      "----------Epoch 29----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 325.462411 CroEtr loss: 0.751309\n",
      "training time:  16.006268978118896\n",
      "\n",
      "----------Epoch 30----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 311.671808 CroEtr loss: 0.750961\n",
      "training time:  16.012603521347046\n",
      "\n",
      "----------Epoch 31----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 315.656849 CroEtr loss: 0.751102\n",
      "training time:  16.01463007926941\n",
      "\n",
      "----------Epoch 32----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 316.318519 CroEtr loss: 0.751106\n",
      "training time:  15.99061632156372\n",
      "\n",
      "----------Epoch 33----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 315.378478 CroEtr loss: 0.751079\n",
      "training time:  16.008910417556763\n",
      "\n",
      "----------Epoch 34----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 307.403907 CroEtr loss: 0.750887\n",
      "training time:  15.998475074768066\n",
      "\n",
      "----------Epoch 35----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 313.656938 CroEtr loss: 0.751027\n",
      "training time:  16.043262481689453\n",
      "\n",
      "----------Epoch 36----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 317.632935 CroEtr loss: 0.751168\n",
      "training time:  16.00624918937683\n",
      "\n",
      "----------Epoch 37----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 309.120726 CroEtr loss: 0.750957\n",
      "training time:  16.025578498840332\n",
      "\n",
      "----------Epoch 38----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 305.549970 CroEtr loss: 0.750852\n",
      "training time:  16.047032594680786\n",
      "\n",
      "----------Epoch 39----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 312.631274 CroEtr loss: 0.751013\n",
      "training time:  16.00752019882202\n",
      "\n",
      "----------Epoch 40----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 314.583304 CroEtr loss: 0.751080\n",
      "training time:  15.998885154724121\n",
      "\n",
      "----------Epoch 41----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 315.574851 CroEtr loss: 0.751116\n",
      "training time:  15.990869522094727\n",
      "\n",
      "----------Epoch 43----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 306.826628 CroEtr loss: 0.750905\n",
      "training time:  16.001338243484497\n",
      "\n",
      "----------Epoch 44----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 308.238515 CroEtr loss: 0.750945\n",
      "training time:  16.004870176315308\n",
      "\n",
      "----------Epoch 45----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 307.502085 CroEtr loss: 0.750924\n",
      "training time:  15.993085145950317\n",
      "\n",
      "----------Epoch 46----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 307.516806 CroEtr loss: 0.750958\n",
      "training time:  16.01358699798584\n",
      "\n",
      "----------Epoch 47----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 302.045273 CroEtr loss: 0.750789\n",
      "training time:  15.994985103607178\n",
      "\n",
      "----------Epoch 48----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 307.634049 CroEtr loss: 0.750918\n",
      "training time:  16.01807737350464\n",
      "\n",
      "----------Epoch 49----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 314.741064 CroEtr loss: 0.751108\n",
      "training time:  16.013027906417847\n",
      "\n",
      "----------Epoch 50----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 303.854213 CroEtr loss: 0.750851\n",
      "training time:  15.999869585037231\n",
      "\n",
      "----------Epoch 51----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 311.997653 CroEtr loss: 0.751043\n",
      "training time:  16.00255823135376\n",
      "\n",
      "----------Epoch 52----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 306.105287 CroEtr loss: 0.750911\n",
      "training time:  16.00366711616516\n",
      "\n",
      "----------Epoch 53----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 311.143440 CroEtr loss: 0.751042\n",
      "training time:  16.018161296844482\n",
      "\n",
      "----------Epoch 54----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 314.111723 CroEtr loss: 0.751117\n",
      "training time:  16.007981538772583\n",
      "\n",
      "----------Epoch 55----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 307.014059 CroEtr loss: 0.750922\n",
      "training time:  16.02276110649109\n",
      "\n",
      "----------Epoch 56----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 307.093387 CroEtr loss: 0.750948\n",
      "training time:  16.006764888763428\n",
      "\n",
      "----------Epoch 57----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 318.419641 CroEtr loss: 0.751213\n",
      "training time:  16.008906841278076\n",
      "\n",
      "----------Epoch 58----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 311.930181 CroEtr loss: 0.751062\n",
      "training time:  15.998748779296875\n",
      "\n",
      "----------Epoch 59----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 300.339107 CroEtr loss: 0.750799\n",
      "training time:  16.00714945793152\n",
      "\n",
      "----------Epoch 60----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 309.393325 CroEtr loss: 0.750997\n",
      "training time:  15.99900197982788\n",
      "\n",
      "----------Epoch 61----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 307.388706 CroEtr loss: 0.750966\n",
      "training time:  16.010441303253174\n",
      "\n",
      "----------Epoch 62----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 315.363982 CroEtr loss: 0.751148\n",
      "training time:  15.996179819107056\n",
      "\n",
      "----------Epoch 63----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 307.311132 CroEtr loss: 0.750978\n",
      "training time:  16.018537044525146\n",
      "\n",
      "----------Epoch 64----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 310.099901 CroEtr loss: 0.751042\n",
      "training time:  15.999111652374268\n",
      "\n",
      "----------Epoch 65----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 306.314581 CroEtr loss: 0.750946\n",
      "training time:  15.997742414474487\n",
      "\n",
      "----------Epoch 66----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 300.143875 CroEtr loss: 0.750774\n",
      "training time:  15.99239730834961\n",
      "\n",
      "----------Epoch 67----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 315.242379 CroEtr loss: 0.751151\n",
      "training time:  15.995429515838623\n",
      "\n",
      "----------Epoch 68----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 309.016474 CroEtr loss: 0.751003\n",
      "training time:  15.997018098831177\n",
      "\n",
      "----------Epoch 69----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 304.830409 CroEtr loss: 0.750913\n",
      "training time:  16.0540132522583\n",
      "\n",
      "----------Epoch 70----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 304.490388 CroEtr loss: 0.750902\n",
      "training time:  16.003097534179688\n",
      "\n",
      "----------Epoch 71----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 303.831598 CroEtr loss: 0.750874\n",
      "training time:  16.027140855789185\n",
      "\n",
      "----------Epoch 73----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 303.928703 CroEtr loss: 0.750887\n",
      "training time:  15.996564865112305\n",
      "\n",
      "----------Epoch 74----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 314.731200 CroEtr loss: 0.751147\n",
      "training time:  15.988793849945068\n",
      "\n",
      "----------Epoch 75----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 311.120400 CroEtr loss: 0.751075\n",
      "training time:  15.983878374099731\n",
      "\n",
      "----------Epoch 76----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 310.811163 CroEtr loss: 0.751057\n",
      "training time:  15.995530843734741\n",
      "\n",
      "----------Epoch 77----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 306.938787 CroEtr loss: 0.750957\n",
      "training time:  16.022586345672607\n",
      "\n",
      "----------Epoch 78----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 312.750296 CroEtr loss: 0.751110\n",
      "training time:  16.010023593902588\n",
      "\n",
      "----------Epoch 79----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 304.830525 CroEtr loss: 0.750908\n",
      "training time:  16.03593397140503\n",
      "\n",
      "----------Epoch 80----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 310.330407 CroEtr loss: 0.751041\n",
      "training time:  16.003912448883057\n",
      "\n",
      "----------Epoch 81----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 305.421312 CroEtr loss: 0.750952\n",
      "training time:  15.996278762817383\n",
      "\n",
      "----------Epoch 82----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 313.450931 CroEtr loss: 0.751130\n",
      "training time:  15.996317625045776\n",
      "\n",
      "----------Epoch 83----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 307.918326 CroEtr loss: 0.750984\n",
      "training time:  16.031882524490356\n",
      "\n",
      "----------Epoch 84----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 305.799602 CroEtr loss: 0.750947\n",
      "training time:  16.01531958580017\n",
      "\n",
      "----------Epoch 85----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 298.602351 CroEtr loss: 0.750748\n",
      "training time:  15.995200395584106\n",
      "\n",
      "----------Epoch 86----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 304.298849 CroEtr loss: 0.750904\n",
      "training time:  16.01366114616394\n",
      "\n",
      "----------Epoch 87----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 303.925990 CroEtr loss: 0.750902\n",
      "training time:  16.004878520965576\n",
      "\n",
      "----------Epoch 88----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 309.483387 CroEtr loss: 0.751046\n",
      "training time:  16.037234783172607\n",
      "\n",
      "----------Epoch 90----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 308.136185 CroEtr loss: 0.751017\n",
      "training time:  16.022718906402588\n",
      "\n",
      "----------Epoch 91----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 300.776071 CroEtr loss: 0.750846\n",
      "training time:  16.043501377105713\n",
      "\n",
      "----------Epoch 92----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 304.791421 CroEtr loss: 0.750934\n",
      "training time:  15.994057416915894\n",
      "\n",
      "----------Epoch 93----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 304.041273 CroEtr loss: 0.750912\n",
      "training time:  16.04329013824463\n",
      "\n",
      "----------Epoch 94----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 314.787588 CroEtr loss: 0.751153\n",
      "training time:  15.997382164001465\n",
      "\n",
      "----------Epoch 95----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 306.491977 CroEtr loss: 0.750973\n",
      "training time:  16.05867624282837\n",
      "\n",
      "----------Epoch 96----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 306.955425 CroEtr loss: 0.750986\n",
      "training time:  16.05597186088562\n",
      "\n",
      "----------Epoch 97----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 303.979836 CroEtr loss: 0.750892\n",
      "training time:  16.06350088119507\n",
      "\n",
      "----------Epoch 98----------\n",
      "Test Results:\n",
      "Accuracy: 99.3% abs-sum loss: 298.436005 CroEtr loss: 0.750789\n",
      "training time:  16.091273546218872\n",
      "\n",
      "----------Epoch 99----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 310.048986 CroEtr loss: 0.751047\n",
      "training time:  16.02324414253235\n",
      "\n",
      "----------Epoch 100----------\n",
      "Test Results:\n",
      "Accuracy: 99.2% abs-sum loss: 305.623599 CroEtr loss: 0.750954\n",
      "training time:  16.09902048110962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 加载数据集\n",
    "    data = pd.read_csv('train.csv')\n",
    "    data = process_data(data)\n",
    "    pred_data = pd.read_csv('testA.csv')\n",
    "    pred_data = get_pred_x(pred_data)\n",
    "\n",
    "    # 初始化模型\n",
    "    adm_lr = 1e-4\n",
    "    sgd_lr = 5e-6\n",
    "    mom = 0.8\n",
    "    w_decay = 1e-5\n",
    "    n_epoch = 100\n",
    "    b_size = 64\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    net = Model()\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(params=net.parameters(), lr=adm_lr, weight_decay=w_decay, amsgrad=True)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    # 拆分训练测试集\n",
    "    train, test = train_test_split(data, test_size=0.2)\n",
    "    train, test = torch.cuda.FloatTensor(train), torch.cuda.FloatTensor(test)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=b_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=b_size)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        if epoch == 20:\n",
    "            optimizer = torch.optim.SGD(params=net.parameters(), lr=sgd_lr, weight_decay=w_decay, momentum=mom)\n",
    "        start = time.time()\n",
    "        print(f\"\\n----------Epoch {epoch + 1}----------\")\n",
    "        train_loop(train_loader, net, loss_fn, optimizer)\n",
    "        test_loop(test_loader, net, loss_fn)\n",
    "        end = time.time()\n",
    "        print('training time: ', end - start)\n",
    "\n",
    "    prediction(net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
